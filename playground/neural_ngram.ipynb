{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-30T08:22:11.724807Z",
     "start_time": "2025-07-30T08:22:11.717987Z"
    }
   },
   "source": [
    "from bpe.byte_pair_encoder import BytePairEncoder\n",
    "\n",
    "def prepare_data(training_data, valid_data, test_data):\n",
    "    bpe_encoder = BytePairEncoder(1000, verbose=False, model_path=\"../models/bpe/model.json\", neural=True)\n",
    "\n",
    "    # Train or load existing BPE\n",
    "    try:\n",
    "        bpe_encoder.load()  # will use model_path\n",
    "        print(\"Loaded existing BPE model.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"No saved BPE found. Training...\")\n",
    "        bpe_encoder.fit(training_data)\n",
    "        bpe_encoder.save()\n",
    "\n",
    "    return len(bpe_encoder.bpe_codes), bpe_encoder.encode(training_data), bpe_encoder.encode(valid_data), bpe_encoder.encode(test_data)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T08:22:12.043356Z",
     "start_time": "2025-07-30T08:22:12.036802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data():\n",
    "    test_string = \"low low low low low lowest lowest newer newer newer newer newer newer wider wider wider new new\"\n",
    "    full_data = open('../data/shakespeare/Shakespeare_clean_full.txt', 'r').read()\n",
    "    training_data = open('../data/shakespeare/Shakespeare_clean_train.txt', 'r').read()\n",
    "    test_data = open('../data/shakespeare/Shakespeare_clean_test.txt', 'r').read()\n",
    "    valid_data = open('../data/shakespeare/Shakespeare_clean_valid.txt', 'r').read()\n",
    "\n",
    "    return test_string, full_data, training_data, test_data, valid_data"
   ],
   "id": "6848d76987f70750",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T08:48:48.085068Z",
     "start_time": "2025-07-30T08:48:48.072202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "class NeuralBigram:\n",
    "\n",
    "    def __init__(self, embedding_dimension, vocab_size, ngram_size, lr=1e-2):\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.vocab_size = vocab_size\n",
    "        self.ngram_size = ngram_size\n",
    "        self.lr = lr\n",
    "\n",
    "        # Embedding: vocab_size → embedding_dimension\n",
    "        self.embedding_matrix = np.random.randn(vocab_size, embedding_dimension) * 0.01\n",
    "\n",
    "        # Linear layer: (context_size * embedding_dim) → vocab_size\n",
    "        input_dim = (self.ngram_size - 1) * self.embedding_dimension\n",
    "        self.linear_W = np.random.randn(input_dim, vocab_size) * 0.01\n",
    "        self.linear_b = np.zeros((1, vocab_size))\n",
    "\n",
    "    def forward(self, x, y=None, target=True):\n",
    "        # Embedding lookup\n",
    "        self.embeddings = self.embedding_matrix[x]  # (B, context_size, D)\n",
    "        self.embeddings_flat = self.embeddings.reshape(x.shape[0], -1)  # (B, context_size*D)\n",
    "\n",
    "        # Linear projection\n",
    "        self.logits = self.embeddings_flat @ self.linear_W + self.linear_b  # (B, vocab_size)\n",
    "\n",
    "        # Softmax\n",
    "        exp_logits = np.exp(self.logits - np.max(self.logits, axis=1, keepdims=True))\n",
    "        self.probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "        if target:\n",
    "            B = x.shape[0]\n",
    "            loss = -np.log(self.probs[np.arange(B), y]).mean()\n",
    "            return loss, self.logits\n",
    "        else:\n",
    "            return self.probs\n",
    "\n",
    "    def backwards(self, x, y):\n",
    "        B = x.shape[0]\n",
    "\n",
    "        # -------- Softmax + Cross-Entropy Gradient --------\n",
    "        dlogits = self.probs.copy()\n",
    "        dlogits[np.arange(B), y] -= 1\n",
    "        dlogits /= B  # (B, vocab_size)\n",
    "\n",
    "        # -------- Gradients for linear layer --------\n",
    "        dW = self.embeddings_flat.T @ dlogits  # (context_size*D, vocab_size)\n",
    "        db = np.sum(dlogits, axis=0, keepdims=True)  # (1, vocab_size)\n",
    "\n",
    "        # Gradient through embeddings_flat\n",
    "        demb_flat = dlogits @ self.linear_W.T  # (B, context_size*D)\n",
    "        demb = demb_flat.reshape(self.embeddings.shape)  # (B, context_size, D)\n",
    "\n",
    "        # Update Embeddings\n",
    "        np.add.at(self.embedding_matrix, x, -self.lr * demb)\n",
    "\n",
    "        # Update weights\n",
    "        self.linear_W -= self.lr * dW\n",
    "        self.linear_b -= self.lr * db\n",
    "\n",
    "    def fit(self, data, epochs=10, batch_size=32, lr_decay=1.0):\n",
    "        \"\"\"\n",
    "        Epoch-based training with shuffling\n",
    "        \"\"\"\n",
    "        data = np.array(data, dtype=np.int64)\n",
    "        num_samples = len(data) - self.ngram_size + 1\n",
    "\n",
    "        # Precompute all n-grams once\n",
    "        contexts = np.stack([data[i:i+self.ngram_size-1] for i in range(num_samples)])\n",
    "        targets = np.array([data[i+self.ngram_size-1] for i in range(num_samples)])\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle data each epoch\n",
    "            perm = np.random.permutation(num_samples)\n",
    "            contexts_shuffled = contexts[perm]\n",
    "            targets_shuffled = targets[perm]\n",
    "\n",
    "            epoch_loss = 0.0\n",
    "            num_batches = int(np.ceil(num_samples / batch_size))\n",
    "\n",
    "            for b in range(num_batches):\n",
    "                start = b * batch_size\n",
    "                end = min((b+1) * batch_size, num_samples)\n",
    "                x_batch = contexts_shuffled[start:end]\n",
    "                y_batch = targets_shuffled[start:end]\n",
    "\n",
    "                # Forward and backward\n",
    "                loss, _ = self.forward(x_batch, y_batch, target=True)\n",
    "                self.backwards(x_batch, y_batch)\n",
    "                epoch_loss += loss * (end - start)\n",
    "\n",
    "            epoch_loss /= num_samples\n",
    "            self.lr *= lr_decay  # Optional learning rate decay\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - loss: {epoch_loss:.4f} - lr: {self.lr:.6f}\")\n",
    "\n",
    "    def perplexity(self, data, batch_size=1) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the perplexity for that model after training on validation data\n",
    "\n",
    "        :param self: the model itfels\n",
    "        :param data: The encoded validation data\n",
    "        :param batch_size: should be 1 perplexity calculation\n",
    "        :return: the perplexity value\n",
    "        \"\"\"\n",
    "\n",
    "        # Load Data and calculate num_samples based on data length and neural_ngram size\n",
    "        data = np.array(data, dtype=np.int64)\n",
    "        num_samples = len(data) - self.ngram_size + 1\n",
    "\n",
    "        contexts = np.stack([data[i:i + self.ngram_size - 1] for i in range(num_samples)])\n",
    "        targets = np.array([data[i + self.ngram_size - 1] for i in range(num_samples)])\n",
    "\n",
    "        nll = 0.0\n",
    "        for i in range(num_samples):\n",
    "            probs = self.forward(contexts[i:i+1], target=False)\n",
    "            nll += -np.log(probs[0, targets[i]])\n",
    "\n",
    "        avg_nll = nll / num_samples\n",
    "        return float(np.exp(avg_nll))"
   ],
   "id": "1fd35c6fb86e4eee",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T11:53:38.969456Z",
     "start_time": "2025-07-30T11:14:25.560276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the data\n",
    "test_string, full_data, training_data, test_data, valid_data = load_data()\n",
    "\n",
    "# Prepare data for neural n-gram\n",
    "vocab_size, train_data, valid_data, test_data = prepare_data(training_data, valid_data, test_data)\n",
    "\n",
    "model = NeuralBigram(embedding_dimension=512, vocab_size=vocab_size, ngram_size=3, lr=0.5)\n",
    "model.fit(train_data, epochs=40, batch_size=32, lr_decay=0.95)\n",
    "print(model.perplexity(valid_data))"
   ],
   "id": "a3c606c2568a9616",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing BPE model.\n",
      "Epoch 1/40 - loss: 5.8980 - lr: 0.475000\n",
      "Epoch 2/40 - loss: 4.8416 - lr: 0.451250\n",
      "Epoch 3/40 - loss: 4.3771 - lr: 0.428687\n",
      "Epoch 4/40 - loss: 4.1708 - lr: 0.407253\n",
      "Epoch 5/40 - loss: 4.0434 - lr: 0.386890\n",
      "Epoch 6/40 - loss: 3.9492 - lr: 0.367546\n",
      "Epoch 7/40 - loss: 3.8732 - lr: 0.349169\n",
      "Epoch 8/40 - loss: 3.8074 - lr: 0.331710\n",
      "Epoch 9/40 - loss: 3.7500 - lr: 0.315125\n",
      "Epoch 10/40 - loss: 3.6993 - lr: 0.299368\n",
      "Epoch 11/40 - loss: 3.6534 - lr: 0.284400\n",
      "Epoch 12/40 - loss: 3.6119 - lr: 0.270180\n",
      "Epoch 13/40 - loss: 3.5732 - lr: 0.256671\n",
      "Epoch 14/40 - loss: 3.5382 - lr: 0.243837\n",
      "Epoch 15/40 - loss: 3.5058 - lr: 0.231646\n",
      "Epoch 16/40 - loss: 3.4759 - lr: 0.220063\n",
      "Epoch 17/40 - loss: 3.4484 - lr: 0.209060\n",
      "Epoch 18/40 - loss: 3.4227 - lr: 0.198607\n",
      "Epoch 19/40 - loss: 3.3987 - lr: 0.188677\n",
      "Epoch 20/40 - loss: 3.3763 - lr: 0.179243\n",
      "Epoch 21/40 - loss: 3.3556 - lr: 0.170281\n",
      "Epoch 22/40 - loss: 3.3362 - lr: 0.161767\n",
      "Epoch 23/40 - loss: 3.3182 - lr: 0.153678\n",
      "Epoch 24/40 - loss: 3.3005 - lr: 0.145995\n",
      "Epoch 25/40 - loss: 3.2848 - lr: 0.138695\n",
      "Epoch 26/40 - loss: 3.2698 - lr: 0.131760\n",
      "Epoch 27/40 - loss: 3.2555 - lr: 0.125172\n",
      "Epoch 28/40 - loss: 3.2419 - lr: 0.118913\n",
      "Epoch 29/40 - loss: 3.2296 - lr: 0.112968\n",
      "Epoch 30/40 - loss: 3.2175 - lr: 0.107319\n",
      "Epoch 31/40 - loss: 3.2064 - lr: 0.101953\n",
      "Epoch 32/40 - loss: 3.1956 - lr: 0.096856\n",
      "Epoch 33/40 - loss: 3.1856 - lr: 0.092013\n",
      "Epoch 34/40 - loss: 3.1764 - lr: 0.087412\n",
      "Epoch 35/40 - loss: 3.1670 - lr: 0.083042\n",
      "Epoch 36/40 - loss: 3.1583 - lr: 0.078890\n",
      "Epoch 37/40 - loss: 3.1506 - lr: 0.074945\n",
      "Epoch 38/40 - loss: 3.1427 - lr: 0.071198\n",
      "Epoch 39/40 - loss: 3.1354 - lr: 0.067638\n",
      "Epoch 40/40 - loss: 3.1285 - lr: 0.064256\n",
      "90.8537634944819\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T11:56:25.199204Z",
     "start_time": "2025-07-30T11:56:11.199421Z"
    }
   },
   "cell_type": "code",
   "source": "model.perplexity(valid_data)",
   "id": "5629129605a7902e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.80881270253751"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T11:59:02.100414Z",
     "start_time": "2025-07-30T11:59:02.075734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "class NeuralBigram:\n",
    "\n",
    "    def __init__(self, embedding_dimension, vocab_size, ngram_size, lr=1e-2, hidden_layer_size = 128):\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.vocab_size = vocab_size\n",
    "        self.ngram_size = ngram_size\n",
    "        self.lr = lr\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "\n",
    "        # Embedding: vocab_size → embedding_dimension\n",
    "        self.embedding_matrix = np.random.randn(vocab_size, embedding_dimension) * 0.01\n",
    "\n",
    "        #Hidden Layer: (context_size * embedding_dim) → hidden layer size\n",
    "        input_dim = (self.ngram_size - 1) * self.embedding_dimension\n",
    "        self.linear_W1 = np.random.randn(input_dim, self.hidden_layer_size) * 0.01\n",
    "        self.linear_b1 = np.zeros((1, self.hidden_layer_size))\n",
    "\n",
    "        # Linear layer: hidden layer size → vocab_size\n",
    "        self.linear_W2 = np.random.randn(self.hidden_layer_size, self.vocab_size) * 0.01\n",
    "        self.linear_b2 = np.zeros((1, self.vocab_size))\n",
    "\n",
    "    def forward(self, x, y=None, target=True):\n",
    "        # Embedding lookup\n",
    "        self.embeddings = self.embedding_matrix[x]  # (B, context_size, D)\n",
    "        self.embeddings_flat = self.embeddings.reshape(x.shape[0], -1)  # (B, context_size*D)\n",
    "\n",
    "        # Hidden layer with tanh activation\n",
    "        self.hidden_layer = self.embeddings_flat @ self.linear_W1 + self.linear_b1\n",
    "        self.hidden_activation = np.tanh(self.hidden_layer)\n",
    "\n",
    "        # Linear projection to vocab size\n",
    "        self.logits = self.hidden_activation @ self.linear_W2 + self.linear_b2  # (B, vocab_size)\n",
    "\n",
    "        # Softmax\n",
    "        exp_logits = np.exp(self.logits - np.max(self.logits, axis=1, keepdims=True))\n",
    "        self.probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "        if target:\n",
    "            B = x.shape[0]\n",
    "            loss = -np.log(self.probs[np.arange(B), y]).mean()\n",
    "            return loss, self.logits\n",
    "        else:\n",
    "            return self.probs\n",
    "\n",
    "    def backwards(self, x, y):\n",
    "        B = x.shape[0]\n",
    "\n",
    "        # -------- Softmax + Cross-Entropy Gradient --------\n",
    "        dlogits = self.probs.copy()\n",
    "        dlogits[np.arange(B), y] -= 1\n",
    "        dlogits /= B  # (B, vocab_size)\n",
    "\n",
    "        # -------- Gradients for output layer (W2, b2) --------\n",
    "        dW2 = self.hidden_activation.T @ dlogits  # (H, V)\n",
    "        db2 = np.sum(dlogits, axis=0, keepdims=True)  # (1, V)\n",
    "\n",
    "        # -------- Backprop into hidden activation --------\n",
    "        dha = dlogits @ self.linear_W2.T  # (B, H)\n",
    "\n",
    "        # -------- Backprop through tanh --------\n",
    "        dh = dha * (1 - self.hidden_activation ** 2)  # (B, H)\n",
    "\n",
    "        # -------- Gradients for hidden layer (W1, b1) --------\n",
    "        dW1 = self.embeddings_flat.T @ dh  # (C*D, H)\n",
    "        db1 = np.sum(dh, axis=0, keepdims=True)  # (1, H)\n",
    "\n",
    "        # -------- Backprop into embeddings --------\n",
    "        demb_flat = dh @ self.linear_W1.T  # (B, C*D)\n",
    "        demb = demb_flat.reshape(self.embeddings.shape)  # (B, C, D)\n",
    "\n",
    "        # -------- Parameter updates --------\n",
    "        # Embeddings\n",
    "        np.add.at(self.embedding_matrix, x, -self.lr * demb)\n",
    "        # Hidden layer\n",
    "        self.linear_W1 -= self.lr * dW1\n",
    "        self.linear_b1 -= self.lr * db1\n",
    "        # Output layer\n",
    "        self.linear_W2 -= self.lr * dW2\n",
    "        self.linear_b2 -= self.lr * db2\n",
    "\n",
    "    def fit(self, data, epochs=10, batch_size=32, lr_decay=1.0):\n",
    "        \"\"\"\n",
    "        Epoch-based training with shuffling\n",
    "        \"\"\"\n",
    "        data = np.array(data, dtype=np.int64)\n",
    "        num_samples = len(data) - self.ngram_size + 1\n",
    "\n",
    "        # Precompute all n-grams once\n",
    "        contexts = np.stack([data[i:i+self.ngram_size-1] for i in range(num_samples)])\n",
    "        targets = np.array([data[i+self.ngram_size-1] for i in range(num_samples)])\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle data each epoch\n",
    "            perm = np.random.permutation(num_samples)\n",
    "            contexts_shuffled = contexts[perm]\n",
    "            targets_shuffled = targets[perm]\n",
    "\n",
    "            epoch_loss = 0.0\n",
    "            num_batches = int(np.ceil(num_samples / batch_size))\n",
    "\n",
    "            for b in range(num_batches):\n",
    "                start = b * batch_size\n",
    "                end = min((b+1) * batch_size, num_samples)\n",
    "                x_batch = contexts_shuffled[start:end]\n",
    "                y_batch = targets_shuffled[start:end]\n",
    "\n",
    "                # Forward and backward\n",
    "                loss, _ = self.forward(x_batch, y_batch, target=True)\n",
    "                self.backwards(x_batch, y_batch)\n",
    "                epoch_loss += loss * (end - start)\n",
    "\n",
    "            epoch_loss /= num_samples\n",
    "            self.lr *= lr_decay  # Optional learning rate decay\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - loss: {epoch_loss:.4f} - lr: {self.lr:.6f}\")\n",
    "\n",
    "    def perplexity(self, data, batch_size=1) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the perplexity for that model after training on validation data\n",
    "\n",
    "        :param self: the model itfels\n",
    "        :param data: The encoded validation data\n",
    "        :param batch_size: should be 1 perplexity calculation\n",
    "        :return: the perplexity value\n",
    "        \"\"\"\n",
    "\n",
    "        # Load Data and calculate num_samples based on data length and neural_ngram size\n",
    "        data = np.array(data, dtype=np.int64)\n",
    "        num_samples = len(data) - self.ngram_size + 1\n",
    "\n",
    "        contexts = np.stack([data[i:i + self.ngram_size - 1] for i in range(num_samples)])\n",
    "        targets = np.array([data[i + self.ngram_size - 1] for i in range(num_samples)])\n",
    "\n",
    "        nll = 0.0\n",
    "        for i in range(num_samples):\n",
    "            probs = self.forward(contexts[i:i+1], target=False)\n",
    "            nll += -np.log(probs[0, targets[i]])\n",
    "\n",
    "        avg_nll = nll / num_samples\n",
    "        return float(np.exp(avg_nll))\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "6e4cb61fc9e4900c",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T12:31:32.896618Z",
     "start_time": "2025-07-30T12:31:32.879152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from neural_ngram.utils.preprocessing import get_ngram_batch\n",
    "\n",
    "class PytorchBigram(nn.Module):\n",
    "    def __init__(self, vocab_size, ngram_size=2, lr=1e-3, device=None):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.ngram_size = ngram_size\n",
    "        self.lr = lr\n",
    "\n",
    "        # Each token directly mapped to logits (like a bigram table)\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "        # Pick device automatically (MPS if available on Mac)\n",
    "        self.device = device or (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        \"\"\"\n",
    "        x: (B, context_size)\n",
    "        targets: (B,) or (B, context_size) depending on neural_ngram\n",
    "        \"\"\"\n",
    "        logits = self.embedding_layer(x)  # (B, context_size, vocab_size)\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B*T, C)      # flatten batch & time\n",
    "        if targets is not None:\n",
    "            targets = targets.view(B*T).to(self.device)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits, None\n",
    "\n",
    "    def fit(self, data, epochs=10, steps_per_epoch=10000, batch_size=32, lr_decay=1.0):\n",
    "        # Optimizer\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0.0\n",
    "\n",
    "            for step in range(steps_per_epoch):\n",
    "                # ---- Random batch ----\n",
    "                x_batch, y_batch = get_ngram_batch(data, n=self.ngram_size, batch_size=batch_size)\n",
    "\n",
    "                # Convert to tensors\n",
    "                x_batch = torch.tensor(x_batch, dtype=torch.long, device=self.device)\n",
    "                y_batch = torch.tensor(y_batch, dtype=torch.long, device=self.device)\n",
    "\n",
    "                # ---- Forward pass ----\n",
    "                _, loss = self.forward(x_batch, targets=y_batch)\n",
    "\n",
    "                # ---- Backward & update ----\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            # Average loss for epoch\n",
    "            epoch_loss /= steps_per_epoch\n",
    "\n",
    "            # Learning rate decay (optional)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] *= lr_decay\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{epochs} - loss: {epoch_loss:.4f} - lr: {optimizer.param_groups[0]['lr']:.6f}\")"
   ],
   "id": "2b49663e6b6a8e85",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T12:44:47.728024Z",
     "start_time": "2025-07-30T12:31:38.018071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = PytorchBigram(vocab_size=vocab_size, ngram_size=2, lr=0.001)\n",
    "model.fit(train_data, epochs=40, batch_size=64, lr_decay=0.99)"
   ],
   "id": "56319566d6291679",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 - loss: 6.3138 - lr: 0.000990\n",
      "Epoch 2/40 - loss: 4.9148 - lr: 0.000980\n",
      "Epoch 3/40 - loss: 4.3785 - lr: 0.000970\n",
      "Epoch 4/40 - loss: 4.1906 - lr: 0.000961\n",
      "Epoch 5/40 - loss: 4.1189 - lr: 0.000951\n",
      "Epoch 6/40 - loss: 4.0985 - lr: 0.000941\n",
      "Epoch 7/40 - loss: 4.0963 - lr: 0.000932\n",
      "Epoch 8/40 - loss: 4.1058 - lr: 0.000923\n",
      "Epoch 9/40 - loss: 4.1035 - lr: 0.000914\n",
      "Epoch 10/40 - loss: 4.1139 - lr: 0.000904\n",
      "Epoch 11/40 - loss: 4.1163 - lr: 0.000895\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[40]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m model = PytorchBigram(vocab_size=vocab_size, ngram_size=\u001B[32m2\u001B[39m, lr=\u001B[32m0.001\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m model.fit(train_data, epochs=\u001B[32m40\u001B[39m, batch_size=\u001B[32m64\u001B[39m, lr_decay=\u001B[32m0.99\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[39]\u001B[39m\u001B[32m, line 57\u001B[39m, in \u001B[36mPytorchBigram.fit\u001B[39m\u001B[34m(self, data, epochs, steps_per_epoch, batch_size, lr_decay)\u001B[39m\n\u001B[32m     55\u001B[39m     optimizer.zero_grad()\n\u001B[32m     56\u001B[39m     loss.backward()\n\u001B[32m---> \u001B[39m\u001B[32m57\u001B[39m     optimizer.step()\n\u001B[32m     59\u001B[39m     epoch_loss += loss.item()\n\u001B[32m     61\u001B[39m \u001B[38;5;66;03m# Average loss for epoch\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/building-gpt/lib/python3.12/site-packages/torch/optim/optimizer.py:493\u001B[39m, in \u001B[36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    488\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    489\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m    490\u001B[39m                 \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    491\u001B[39m             )\n\u001B[32m--> \u001B[39m\u001B[32m493\u001B[39m out = func(*args, **kwargs)\n\u001B[32m    494\u001B[39m \u001B[38;5;28mself\u001B[39m._optimizer_step_code()\n\u001B[32m    496\u001B[39m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/building-gpt/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001B[39m, in \u001B[36m_use_grad_for_differentiable.<locals>._use_grad\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m     89\u001B[39m     torch.set_grad_enabled(\u001B[38;5;28mself\u001B[39m.defaults[\u001B[33m\"\u001B[39m\u001B[33mdifferentiable\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m     90\u001B[39m     torch._dynamo.graph_break()\n\u001B[32m---> \u001B[39m\u001B[32m91\u001B[39m     ret = func(\u001B[38;5;28mself\u001B[39m, *args, **kwargs)\n\u001B[32m     92\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m     93\u001B[39m     torch._dynamo.graph_break()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/building-gpt/lib/python3.12/site-packages/torch/optim/adam.py:244\u001B[39m, in \u001B[36mAdam.step\u001B[39m\u001B[34m(self, closure)\u001B[39m\n\u001B[32m    232\u001B[39m     beta1, beta2 = group[\u001B[33m\"\u001B[39m\u001B[33mbetas\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m    234\u001B[39m     has_complex = \u001B[38;5;28mself\u001B[39m._init_group(\n\u001B[32m    235\u001B[39m         group,\n\u001B[32m    236\u001B[39m         params_with_grad,\n\u001B[32m   (...)\u001B[39m\u001B[32m    241\u001B[39m         state_steps,\n\u001B[32m    242\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m244\u001B[39m     adam(\n\u001B[32m    245\u001B[39m         params_with_grad,\n\u001B[32m    246\u001B[39m         grads,\n\u001B[32m    247\u001B[39m         exp_avgs,\n\u001B[32m    248\u001B[39m         exp_avg_sqs,\n\u001B[32m    249\u001B[39m         max_exp_avg_sqs,\n\u001B[32m    250\u001B[39m         state_steps,\n\u001B[32m    251\u001B[39m         amsgrad=group[\u001B[33m\"\u001B[39m\u001B[33mamsgrad\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m    252\u001B[39m         has_complex=has_complex,\n\u001B[32m    253\u001B[39m         beta1=beta1,\n\u001B[32m    254\u001B[39m         beta2=beta2,\n\u001B[32m    255\u001B[39m         lr=group[\u001B[33m\"\u001B[39m\u001B[33mlr\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m    256\u001B[39m         weight_decay=group[\u001B[33m\"\u001B[39m\u001B[33mweight_decay\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m    257\u001B[39m         eps=group[\u001B[33m\"\u001B[39m\u001B[33meps\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m    258\u001B[39m         maximize=group[\u001B[33m\"\u001B[39m\u001B[33mmaximize\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m    259\u001B[39m         foreach=group[\u001B[33m\"\u001B[39m\u001B[33mforeach\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m    260\u001B[39m         capturable=group[\u001B[33m\"\u001B[39m\u001B[33mcapturable\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m    261\u001B[39m         differentiable=group[\u001B[33m\"\u001B[39m\u001B[33mdifferentiable\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m    262\u001B[39m         fused=group[\u001B[33m\"\u001B[39m\u001B[33mfused\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m    263\u001B[39m         grad_scale=\u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mgrad_scale\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[32m    264\u001B[39m         found_inf=\u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mfound_inf\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[32m    265\u001B[39m     )\n\u001B[32m    267\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/building-gpt/lib/python3.12/site-packages/torch/optim/optimizer.py:154\u001B[39m, in \u001B[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    152\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m disabled_func(*args, **kwargs)\n\u001B[32m    153\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m154\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m func(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/building-gpt/lib/python3.12/site-packages/torch/optim/adam.py:876\u001B[39m, in \u001B[36madam\u001B[39m\u001B[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[39m\n\u001B[32m    873\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    874\u001B[39m     func = _single_tensor_adam\n\u001B[32m--> \u001B[39m\u001B[32m876\u001B[39m func(\n\u001B[32m    877\u001B[39m     params,\n\u001B[32m    878\u001B[39m     grads,\n\u001B[32m    879\u001B[39m     exp_avgs,\n\u001B[32m    880\u001B[39m     exp_avg_sqs,\n\u001B[32m    881\u001B[39m     max_exp_avg_sqs,\n\u001B[32m    882\u001B[39m     state_steps,\n\u001B[32m    883\u001B[39m     amsgrad=amsgrad,\n\u001B[32m    884\u001B[39m     has_complex=has_complex,\n\u001B[32m    885\u001B[39m     beta1=beta1,\n\u001B[32m    886\u001B[39m     beta2=beta2,\n\u001B[32m    887\u001B[39m     lr=lr,\n\u001B[32m    888\u001B[39m     weight_decay=weight_decay,\n\u001B[32m    889\u001B[39m     eps=eps,\n\u001B[32m    890\u001B[39m     maximize=maximize,\n\u001B[32m    891\u001B[39m     capturable=capturable,\n\u001B[32m    892\u001B[39m     differentiable=differentiable,\n\u001B[32m    893\u001B[39m     grad_scale=grad_scale,\n\u001B[32m    894\u001B[39m     found_inf=found_inf,\n\u001B[32m    895\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/building-gpt/lib/python3.12/site-packages/torch/optim/adam.py:476\u001B[39m, in \u001B[36m_single_tensor_adam\u001B[39m\u001B[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001B[39m\n\u001B[32m    474\u001B[39m         denom = (max_exp_avg_sqs[i].sqrt() / bias_correction2_sqrt).add_(eps)\n\u001B[32m    475\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m476\u001B[39m         denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)\n\u001B[32m    478\u001B[39m     param.addcdiv_(exp_avg, denom, value=-step_size)\n\u001B[32m    480\u001B[39m \u001B[38;5;66;03m# Lastly, switch back to complex view\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "59582792a326a12f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
